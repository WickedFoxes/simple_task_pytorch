seed: 42
device: "cuda"

dataset:
  name: "imdb"
  root: "./data/imdb"
  pretrained_tokenizer_name: "bert-base-uncased"
  num_workers: 8
  batch_size: 64
  pin_memory: true
  max_len: 400

model:
  name: "lstm_attention_classifier"
  vocab_size: 30522 # tokenizer의 vocab_size
  pad_idx: 0 # tokenizer의 pad_idx
  embed_dim: 100
  hidden_dim: 256
  num_layers: 2
  bidirectional: True
  dropout: 0.5
  num_classes: 2

optimizer:
  name: "adam"
  lr: 0.001
  weight_decay: 0.0001

scheduler:
  name: "reduce_lr_on_plateau"
  mode: "min"
  factor: 0.1
  patience: 3

loss:
  name: "cross_entropy"

logger:
  name: "simple"

trainer:
  name : "classifier_trainer"
  epochs: 20
  grad_clip: 1.0
  amp: true             # mixed precision
  val_interval: 1       # 에폭 단위 검증
  save_best: true

checkpoint_saver:
  save_dir: "/content/drive/MyDrive/Colab_sample_checkpoints/bilstm/lstm_attention_classifier_imdb"
  save_best: true
  save_last: false

early_stop:
  patience: 10