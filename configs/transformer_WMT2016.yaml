seed: 42
device: "cuda"

dataset:
  name: "wmt16_de_en"
  root: "./data/wmt16_de_en"
  pretrained_tokenizer_name: "xlm-roberta-base"
  num_workers: 4
  batch_size: 32
  pin_memory: true
  max_len: 128


model:
  name: "transformer_tl"
  src_vocab_size: 250002 # tokenizer의 vocab_size
  tgt_vocab_size: 250002 # tokenizer의 vocab_size
  src_pad_id: 1 # tokenizer의 pad_idx
  tgt_pad_id: 1 # tokenizer의 pad_idx
  embed_dim: 256
  num_heads: 4
  num_encoder_layers: 3
  num_decoder_layers: 3
  dropout_p: 0.1
  feedforward_dim: 512

optimizer:
  name: "adamw"
  lr: 3e-4
  weight_decay: 0.01

scheduler:
  name: "cosine_schedule_with_warmup"
  num_warmup_steps: 35538
  num_training_steps: 710760
  num_cycles: 1

loss:
  name: "label_smoothing"
  label_smoothing: 0.1
  ignore_index: 1 # tokenizer의 pad_idx

logger:
  name: "simple"

trainer:
  name : "translate_trainer"
  epochs: 5
  grad_clip: 1.0
  amp: true             # mixed precision
  val_interval: 1       # 에폭 단위 검증
  save_best: true

checkpoint_saver:
  save_dir: "checkpoints/wmt16_de_en/transformer_tl"
  save_best: true
  save_last: false